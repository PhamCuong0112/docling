import os
import streamlit as st
import pandas as pd
from docling.document_converter import DocumentConverter
import chromadb
from sentence_transformers import SentenceTransformer
import tempfile
from tqdm import tqdm
from dotenv import load_dotenv
import requests
import json

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "llama2")

class OllamaClient:
    def __init__(self, host, model):
        self.host = host
        self.model = model
        self.api_generate = f"{host}/api/generate"
    
    def generate(self, prompt, stream=True, **kwargs):
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            **kwargs
        }
        
        response = requests.post(self.api_generate, headers=headers, json=data, stream=stream)
        response.raise_for_status()
        
        if stream:
            for line in response.iter_lines():
                if line:
                    json_response = json.loads(line)
                    yield json_response
        else:
            yield json.loads(response.text)

# Streamlitã®è¨­å®š
st.set_page_config(
    page_title="Excel RAG Assistant",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .excel-preview {
        background: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.title("ğŸ”§ è¨­å®š")
      TEMPERATURE = st.slider(
        "æ¸©åº¦ï¼ˆå‰µé€ æ€§ï¼‰",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        help="å€¤ãŒé«˜ã„ã»ã©å‰µé€ çš„ãªå›ç­”ã«ãªã‚Šã¾ã™"
    )

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
@st.cache_resource
def initialize_models():
    try:
        converter = DocumentConverter()
        llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=4096,
            n_threads=8,
            verbose=False
        )
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        chroma_client = chromadb.Client()
        return converter, llm, embedding_model, chroma_client
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None, None

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
def main():
    st.title("ğŸ“Š Excel RAG Assistant powered by Docling & Llama-3.2")
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    converter, llm, embedding_model, chroma_client = initialize_models()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["xlsx", "xls"],
        help="200MBä»¥ä¸‹ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_file:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ™‚ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                file_path = tmp_file.name
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®è¡¨ç¤º
            with st.expander("Excel ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=True):
                df = pd.read_excel(file_path)
                st.dataframe(df, use_container_width=True)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
            with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­..."):
                # Doclingã§ã®å¤‰æ›
                result = converter.convert(file_path)
                
                # ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
                collection_name = "excel_data_" + os.path.splitext(uploaded_file.name)[0]
                try:
                    collection = chroma_client.create_collection(
                        name=collection_name,
                        embedding_function=lambda texts: embedding_model.encode(texts).tolist()
                    )
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã¨ä¿å­˜
                    chunks = result.document.export_to_markdown().split("\n\n")
                    
                    with st.progress(0) as progress_bar:
                        for i, chunk in enumerate(chunks):
                            collection.add(
                                documents=[chunk],
                                ids=[f"chunk_{i}"]
                            )
                            progress_bar.progress((i + 1) / len(chunks))
                    
                    st.success("âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    return
            
            # ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
            
            if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                with st.chat_message("assistant"):
                    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        # é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œç´¢
                        results = collection.query(
                            query_texts=[prompt],
                            n_results=3
                        )
                        
                        context = "\n".join(results["documents"][0])
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
                        system_prompt = """ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
                        æä¾›ã•ã‚ŒãŸExcelãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
                        ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚"""
                        
                        full_prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ï¼š

{system_prompt}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š
{context}

è³ªå•ï¼š{prompt}

å›ç­”ï¼š"""
                        
                        # å›ç­”ã®ç”Ÿæˆ
                        response = llm(
                            full_prompt,
                            max_tokens=MAX_TOKENS,
                            temperature=TEMPERATURE,
                            stream=True
                        )
                        
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
                        placeholder = st.empty()
                        full_response = ""
                        for chunk in response:
                            if chunk["choices"][0]["text"]:
                                full_response += chunk["choices"][0]["text"]
                                placeholder.markdown(full_response)
                        
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": full_response
                        })
            
            # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
            if st.sidebar.button("ğŸ’« ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
                st.session_state.messages = []
                collection.delete()
                st.experimental_rerun()
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            if "file_path" in locals():
                os.unlink(file_path)

if __name__ == "__main__":
    main()
