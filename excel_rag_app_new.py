import os
import streamlit as st
import pandas as pd
from docling.document_converter import DocumentConverter
import chromadb
from chromadb.api.types import Documents, EmbeddingFunction, Embeddings
import tempfile
from tqdm import tqdm
from dotenv import load_dotenv
import requests
import json
import numpy as np
from typing import List
import torch
import asyncio
from sentence_transformers import SentenceTransformer
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# PyTorchã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—è¨­å®š
if torch.cuda.is_available():
    torch.cuda.set_device(0)
    
# asyncioã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—è¨­å®š
try:
    loop = asyncio.get_event_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .excel-preview {
        background: #f0f2f6;
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "llama2")

class OllamaClient:
    def __init__(self, host, model):
        self.host = host
        self.model = model
        self.api_generate = f"{host}/api/generate"
    
    def generate(self, prompt, stream=True, **kwargs):
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            **kwargs
        }
        
        response = requests.post(self.api_generate, headers=headers, json=data, stream=stream)
        response.raise_for_status()
        
        if stream:
            for line in response.iter_lines():
                if line:
                    json_response = json.loads(line)
                    yield json_response
        else:
            yield json.loads(response.text)

# ã‚«ã‚¹ã‚¿ãƒ Embeddingé–¢æ•°ã®å®šç¾©
class SentenceTransformerEmbedding(EmbeddingFunction):
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
        if torch.cuda.is_available():
            self.model = self.model.to('cuda')
    
    def __call__(self, input: Documents) -> Embeddings:
        with torch.no_grad():
            embeddings = self.model.encode(input, convert_to_numpy=True, show_progress_bar=False)
            return embeddings.tolist()
            
    @property
    def device(self):
        return next(self.model.parameters()).device

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def initialize_models():
    converter = None
    ollama_client = None
    embedding_function = None
    chroma_client = None
    
    # DoclingåˆæœŸåŒ–
    try:
        converter = DocumentConverter()
        st.info("DoclingåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        st.error(f"DoclingåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None, None
    
    # Ollama ClientåˆæœŸåŒ–
    try:
        ollama_client = OllamaClient(OLLAMA_HOST, MODEL_NAME)
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        next(ollama_client.generate("test", stream=False))
        st.info("Ollama ClientåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        st.error(f"Ollama ClientåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None, None
    
    # Embedding ModelåˆæœŸåŒ–
    try:
        embedding_function = SentenceTransformerEmbedding()
        st.info("Embedding ModelåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        st.error(f"Embedding ModelåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None, None
    
    # ChromaDBåˆæœŸåŒ–
    try:
        os.makedirs("./chroma_db", exist_ok=True)
        chroma_client = chromadb.PersistentClient(path="./chroma_db")
        st.info("ChromaDBåˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        st.error(f"ChromaDBåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None, None, None, None
        
    return converter, ollama_client, embedding_function, chroma_client

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
def main():
    st.title("ğŸ“Š Excel RAG Assistant powered by Docling & Llama2")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.title("ğŸ”§ è¨­å®š")
        
        TEMPERATURE = st.slider(
            "æ¸©åº¦ï¼ˆå‰µé€ æ€§ï¼‰",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            help="å€¤ãŒé«˜ã„ã»ã©å‰µé€ çš„ãªå›ç­”ã«ãªã‚Šã¾ã™"
        )
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    converter, ollama_client, embedding_function, chroma_client = initialize_models()
    if None in (converter, ollama_client, embedding_function, chroma_client):
        st.error("åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    st.success("ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")

    if not all([converter, ollama_client, embedding_function, chroma_client]):
        st.error("ä¸€éƒ¨ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°ã‚„ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=["xlsx", "xls"],
        help="200MBä»¥ä¸‹ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_file:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ™‚ä¿å­˜
            with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                file_path = tmp_file.name
              # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã‚°ãƒ©ãƒ•ã®è¡¨ç¤º
            with st.expander("Excel ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ & ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–", expanded=True):
                df = pd.read_excel(file_path)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®è¡¨ç¤º
                st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«")
                st.dataframe(df, use_container_width=True)
                
                # ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                st.subheader("ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–")
                
                # æ•°å€¤åˆ—ã‚’è‡ªå‹•æ¤œå‡º
                numeric_columns = df.select_dtypes(include=[np.number]).columns
                
                if len(numeric_columns) > 0:
                    # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—ã®é¸æŠ
                    chart_type = st.selectbox(
                        "ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒ—ã‚’é¸æŠ",
                        ["æ£’ã‚°ãƒ©ãƒ•", "æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•", "æ•£å¸ƒå›³", "ç®±ã²ã’å›³", "ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ "]
                    )
                    
                    # Xè»¸ã¨Yè»¸ã®é¸æŠ
                    x_column = st.selectbox("Xè»¸ã‚’é¸æŠ", df.columns)
                    y_column = st.selectbox("Yè»¸ã‚’é¸æŠ", numeric_columns)
                    
                    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
                    try:
                        fig = None
                        if chart_type == "æ£’ã‚°ãƒ©ãƒ•":
                            fig = px.bar(df, x=x_column, y=y_column, title=f"{x_column} vs {y_column}")
                        elif chart_type == "æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•":
                            fig = px.line(df, x=x_column, y=y_column, title=f"{x_column} vs {y_column}")
                        elif chart_type == "æ•£å¸ƒå›³":
                            fig = px.scatter(df, x=x_column, y=y_column, title=f"{x_column} vs {y_column}")
                        elif chart_type == "ç®±ã²ã’å›³":
                            fig = px.box(df, x=x_column, y=y_column, title=f"{x_column} ã«ã‚ˆã‚‹ {y_column} ã®åˆ†å¸ƒ")
                        elif chart_type == "ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ":
                            fig = px.histogram(df, x=y_column, title=f"{y_column} ã®åˆ†å¸ƒ")
                        
                        if fig:
                            # ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
                            fig.update_layout(
                                width=800,
                                height=500,
                                template="plotly_white"
                            )
                            st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.error(f"ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                else:
                    st.warning("æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ã«ã¯æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
            with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­..."):
                # converterãŒNoneã§ãªã„ã‹ç¢ºèª
                if converter is None:
                    st.error("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤‰æ›å™¨ï¼ˆconverterï¼‰ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚„ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    return
                # Doclingã§ã®å¤‰æ›
                result = converter.convert(file_path)
                  # ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
                collection_name = "excel_data_" + os.path.splitext(uploaded_file.name)[0]
                try:
                    # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç¢ºèª
                    if chroma_client is None:
                        st.error("ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                        return

                    # æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å‰Šé™¤
                    try:
                        chroma_client.delete_collection(name=collection_name)
                    except Exception:
                        pass  # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–
                    
                    # æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                    collection = chroma_client.create_collection(
                        name=collection_name,
                        embedding_function=embedding_function
                    )
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã¨ä¿å­˜
                    chunks = result.document.export_to_markdown().split("\n\n")
                    
                    progress_bar = st.progress(0)
                    for i, chunk in enumerate(chunks):
                        if chunk.strip():  # ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                            collection.add(
                                documents=[chunk],
                                ids=[f"chunk_{i}"]
                            )
                            progress_bar.progress((i + 1) / len(chunks))
                    
                    st.success("âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    return
            
            # ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
            if "messages" not in st.session_state:
                st.session_state.messages = []
            
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
            
            if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                with st.chat_message("assistant"):
                    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        if ollama_client is None:
                            st.error("Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                            return

                        # é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œç´¢
                        results = collection.query(
                            query_texts=[prompt],
                            n_results=3
                        )
                        
                        # å®‰å…¨ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                        documents = results.get("documents")
                        if documents and len(documents) > 0 and documents[0]:
                            context = "\n".join(documents[0])
                        else:
                            context = "é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
                        system_prompt = """ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
                        æä¾›ã•ã‚ŒãŸExcelãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
                        ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ãã®æ—¨ã‚’æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚"""
                        
                        full_prompt = f"""{system_prompt}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š
{context}

è³ªå•ï¼š{prompt}

å›ç­”ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"""

                        response_message = ""
                        message_placeholder = st.empty()
                        
                        try:
                            for response in ollama_client.generate(
                                prompt=full_prompt,
                                stream=True,
                                temperature=TEMPERATURE
                            ):
                                if "response" in response:
                                    response_message += response["response"]
                                    message_placeholder.markdown(response_message + "â–Œ")
                            
                            message_placeholder.markdown(response_message)
                        except Exception as e:
                            st.error(f"å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            
            # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
            if st.sidebar.button("ğŸ’« ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
                st.session_state.messages = []
                collection.delete()
                st.rerun()
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            if "file_path" in locals():
                os.unlink(file_path)

if __name__ == "__main__":
    main()
