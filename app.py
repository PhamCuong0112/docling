import streamlit as st
from docling.document_converter import DocumentConverter
from llama_cpp import Llama
import chromadb
from sentence_transformers import SentenceTransformer
import pandas as pd
import os
import tempfile

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š
st.set_page_config(
    page_title="Docling RAG ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“„",
    layout="wide"
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
st.sidebar.title("è¨­å®š")

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
MODEL_PATH = st.sidebar.text_input(
    "Llamaãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹",
    "models/llama-2-7b-chat.gguf",
    help="Llama-2ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
)

# åˆæœŸåŒ–
@st.cache_resource
def initialize_models():
    converter = DocumentConverter()
    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,
        n_threads=4
    )
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    chroma_client = chromadb.Client()
    return converter, llm, embedding_model, chroma_client

converter, llm, embedding_model, chroma_client = initialize_models()

# ãƒ¡ã‚¤ãƒ³UI
st.title("Docling RAG ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx", "xls", "pdf", "docx"])

if uploaded_file:
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        file_path = tmp_file.name

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤‰æ›
    try:
        with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­..."):
            result = converter.convert(file_path)
            
            # ChromaDBã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            collection = chroma_client.create_collection(
                name="document_chunks",
                embedding_function=lambda texts: embedding_model.encode(texts).tolist()
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ä¿å­˜
            chunks = result.document.export_to_markdown().split("\n\n")
            collection.add(
                documents=chunks,
                ids=[f"chunk_{i}" for i in range(len(chunks))]
            )
            
            st.success("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        os.unlink(file_path)

# ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # é–¢é€£æ–‡æ›¸ã®æ¤œç´¢
        results = collection.query(
            query_texts=[prompt],
            n_results=3
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
        context = "\n".join(results["documents"][0])
        
        # Llamaã§RAGå¿œç­”ã‚’ç”Ÿæˆ
        response = llm(
            f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {prompt}

å›ç­”ï¼š""",
            max_tokens=512,
            temperature=0.7
        )
        
        st.markdown(response["choices"][0]["text"])
        st.session_state.messages.append({"role": "assistant", "content": response["choices"][0]["text"]})

# ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
if st.sidebar.button("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢"):
    st.session_state.messages = []
    if "collection" in locals():
        collection.delete()
    st.experimental_rerun()
